---
title: "Assignment 4"
output: html_document
date: "2025-05-13"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load Packages
```{r}
pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes, rstan, 
               ggplot2, dplyr, patchwork, bayesplot)
```

# EMPERICAL DATA
## Load Emperical Data and Clean it
Data used contains: 
- *The 'dangerous' category* (not 'nutrisious')
- *Condition 1* dyads (not condition 2)
- *Session 1* danger: aliens with spots AND eyes on stalks (not sessions 2 and 3)
- *Non-test rows* 
```{r}
#emp_data <- read.delim("data/AlienData.txt", sep = ",")
emp_data <- read.delim("AlienData.csv", sep = ",")
emp_data$subject <- as.factor(emp_data$subject)  # make subject categorical
emp_data <- emp_data[emp_data$session == 1 &     # incl only session 1
                       emp_data$condition == 1 & # incl only condition 1
                       emp_data$test == 0,]      # remove test rows
# remove columns not used
emp_data <- emp_data %>% 
  select(-c(condition, session, test, nutricious, RT, motivation, # remove none needed columns
            competence, communication, complement)) %>% 
  group_by(subject) %>%                                           # group the data frame by subjects
  mutate(performance = cumsum(correct) / seq_along(correct))      # create a performance column 
```

## Expand Features
Stimulus column contains the alien-file names. The features are indicated by a 0 or 1 in a set position: 
*1. eyes*: on stalks = 1, not on stalks = 0
*2. legs*: small = 1, big = 0
*3. spots*: spots = 1, no spots = 0
*4. arms*: up = 1, down = 0
*5. color*: green = 1, blue = 0
The five features are added as binary columns
```{r}
emp_data[,c("eyes", "legs", "spots", "arms", "color")] <- str_split_fixed(
  emp_data$stimulus, pattern = "", n = 6)[,1:5]
emp_data <- emp_data %>% 
  mutate_at(c("eyes", "legs", "spots", "arms", "color"), as.integer)
```

## Plot the Emperical Data
### Cumulative Scores and Performance for each Participant 
Calculate mean cumulative value per trial and plot it along each subjects
```{r}
# Calculate mean cumulative and performance for each trial
mean_emp <- emp_data %>% 
  group_by(trial) %>% 
  summarise(mean_cum = mean(cumulative)/100, mean_perf = mean(performance))
# Plot the cumulative scores
p1 <- ggplot()+
  geom_line(data = emp_data, aes(group = subject, x = trial, y = cumulative/100), 
            color = "grey")+
  geom_line(data = mean_emp, aes(x = trial, y = mean_cum), color = "purple3")+
  annotate(geom="text", x= nrow(mean_emp)+4, 
           y= mean_emp$mean_cum[mean_emp$trial == nrow(mean_emp)], 
           label="Mean", color = "purple3", size = 3) +
  ggtitle("Emperical Data Cumulative Score")+
  labs(y = "cumulative")+
  theme_minimal()
# Plot the performance 
p2 <- ggplot(emp_data)+
  geom_line(aes(x = trial, y = performance), color = "grey")+
  geom_line(data = mean_emp, aes(x = trial, y = mean_perf), color = "green4")+
  annotate(geom="text", x= nrow(mean_emp)+4, 
           y= mean_emp$mean_perf[mean_emp$trial == nrow(mean_emp)], 
           label="Mean", color = "green4", size = 3) +
  ggtitle("Emperical Data Performance")+
  theme_minimal()
p1/p2
ggsave(filename = "figs/emperical_data_performance.png") #save in figs folder
```

### Percentage of Correct Categorisation per Stimuli
```{r}
# Calculate summary stats for each stimuli 
emp_data_sum <- pivot_wider(
  emp_data[c("stimulus", "correct", "dangerous", "cycle")], 
  names_from = correct, values_from = correct, values_fn = length, values_fill = 0)
colnames(emp_data_sum) <- c("stimuli", "danger", "cycle", "wrong", "correct")
emp_data_sum$stimuli <- str_split_fixed(emp_data_sum$stimuli, 
                                        pattern = ".jpg", n = 2)[,1]
emp_data_sum <- emp_data_sum %>% 
  group_by(cycle) %>% 
  arrange(desc(danger)) %>% 
  group_by(cycle, stimuli) %>% 
  mutate(n = (wrong+correct)) %>% 
  mutate(perc_corr = (correct/n)*100) %>% 
  mutate_at(c("stimuli", "cycle"), as.factor)
# Plot it
ggplot(emp_data_sum) +
  geom_point(aes(x = cycle, y = stimuli,  size = perc_corr, fill = perc_corr), 
               alpha=0.7, shape = 21, position = position_jitter(width = 0.15)) + 
  scale_fill_viridis_c(breaks = c(25, 50, 75, 100), guide = "legend", name="% Correct")+ 
  scale_size_continuous(range = c(.5, 10),  name="% Correct", 
                        breaks = c(25, 50, 75, 100), guide = "legend")+
  theme_minimal()+ 
  facet_grid(row = "danger", scales = "free_y", space = "free_y", labeller = label_both)+
  theme(panel.grid = element_blank(), panel.spacing = unit(1.5, "lines"))+ 
  coord_cartesian(clip = "off")
```


# FUNCTIONS 
## Experiment Function
The function implements the structure of the experiment, and returns a df with the order of the stimuli for a given number of cycles. It has the following parameters: 
*agent_id*: the id for the given agent
*w_key*: some string indicating what weights are being used. 
*c*: The sensitivity scale used. 
*stimuli*: list of unique stimuli (the 32 alien-filenames)
*cycles*: the number of times the stimuli should be repeated (3 cycles)
*danger_fun*: how the 'danger' category is implemented (so far just the 'ADD' option)
*danger_features*: the features indicating danger
*danger_vals*: the values each of the danger features should have to be dangerous. 

The function returns a dataframe with nrows = number of stimuli * cycles. A stimuli column, a cycle column and a danger column. 
```{r}
experiment <- function(agent_id, w_key, c, stimuli, cycles, danger_fun = "ADD", 
                       danger_features, danger_vals){
  trials <- length(stimuli)*cycles
  n_stim <- length(stimuli)
  stim_df <- data.frame(w = rep(w_key, trials), c = rep(c, trials), 
                        agent_id = rep(agent_id, trials), trial = 1:trials, 
                        cycle = rep(NA, trials), stimuli = rep(NA, trials), 
                        danger = rep(NA, trials))
  for(cycle in 1:cycles){
    stim_order <- sample(stimuli, size = n_stim, replace = F)
    for(i in 1:n_stim){
      n_trial <- i+((cycle-1) *n_stim)
      stim_df[n_trial, "cycle"] <- cycle
      stim_df[n_trial , "stimuli"] <- stim_order[i]
    }
  } 
  stim_df[,c("eyes", "legs", "spots", "arms", "color")] <- str_split_fixed(
    stim_df$stimuli, pattern = "", n = 5)
  if(danger_fun == "ADD"){
    stim_df$danger <- ifelse(stim_df[danger_features[1]] == danger_vals[1] & 
                             stim_df[danger_features[2]] == danger_vals[2], 1, 0)[,1]
  }
  else{
    print("The danger function is not implemented in the experiment function")
  }
  stim_df <- stim_df %>% mutate_at(c("danger", "eyes", "legs", "spots", "arms", "color"), as.integer)
  return(stim_df)
}
```

## Softmax Function
Softmax function to generate weights for the features that sum to 1 
```{r}
softmax <- function(vector){
  return(exp(vector)/sum(exp(vector)))
}
```

## General Context Model Functions
### Distance & Similarity
```{r}
distance <- function(vect1, vect2, w) {
  return(sum(w * abs(vect1 - vect2)))
}

similarity <- function(distance, c) {
  return(exp(-c * distance))
}

```

### Agent Functions 
- *w*       : Attention weight (attention to parameter dimensions, sums to 1)
- *c*       : Sensitivity parameter (determines how quickly similarity decreases with distance)
- *obs*     : Obervations (list of simuli, one for each trial)
- *cat_list*: List of 0/1 for each trial (1 = danger, 0 = not danger) 
```{r}
gcm_agent <- function(w, c, obs, cat_list){
  cat_probs <- c()     # list of probabilities of selecting '1' (danger) for each trial
  ntrials <- nrow(obs) # number of trials
  for (i in 1:ntrials){
    # for first trial and if the category hasn't been seen yet: 
    if (i == 1 || sum(cat_list[1:(i - 1)]) == 0 || sum(cat_list[1:(i - 1)]) == (i - 1)) {
      cat_probs <- c(cat_probs, 0.5) # random guess 
    }
    else {
      similarities <- c() # list of similarities between current simuli and all previous
      for (e in 1:(i - 1)) {
        dist <- distance(obs[i, ], obs[e, ], w)              # calculate distance
        similarities <- c(similarities, similarity(dist, c)) # caluclate similarity
      }
      # probability of categorising i as dangerous: 
      numerator <- mean(similarities[cat_list[1:(i - 1)] == 1])
      denominator <- mean(similarities[cat_list[1:(i - 1)] == 1]) + mean(
        similarities[cat_list[1:(i - 1)] == 0])
      cat_probs <- c(cat_probs, numerator / denominator)
    }
  }
  choices <- rbinom(ntrials, 1, cat_probs)
  return(choices)
}
```

# SIMULATION
## Simulate Agents
```{r}
# Define three sets of weights for the features to test
weights <- list("even" = softmax(rep(1, 5)),                 # 0.2 0.2 0.2 0.2 0.2
                "eyes_spots" = softmax(c(1.8, 1, 1.8, 1, 1)),# 0.299 0.134 0.299 0.134 0.134
                "color_arms" = softmax(c(1, 1, 1, 1.8, 1.8)))# 0.134 0.134 0.134 0.299 0.299

c <- 1.5                           # sensitivity parameter (same for all agents)
cycles <- 3                        # number of times stimuli is repeated
n_subA <- 10                       # number of agents per weight dists 
n_agents <- n_subA*length(weights) # total number of agents to be simulated: 10 with each of the weight dists 

stim <- unique(str_split_fixed(emp_data$stimulus, pattern = ".jpg", n = 2)[,1]) # define unique stimuli

d_features <- c("eyes", "spots") # the features that with d_vals indicate danger
d_vals <- c(1, 1)                # the values of d_features for them to indicate danger

# Simulate n_agents with the given parameters: 

for (j in 1:length(weights)){
  w <- weights[[j]] # vector of weights for each feature
  for(i in (j*n_subA-(n_subA-1)):(j*n_subA)){ #go through ten agents for each weight vect (1: 1-10, 2: 11-20, 3: 21-30)
    # create exp data frame and unique stim order for given agent: 
    exp_df <- experiment(agent_id = i, w = names(weights)[j], c = c, 
                         stimuli = stim , cycles = cycles, danger_fun = "ADD",
                         danger_features = d_features, danger_vals = d_vals)
    # generate choices for the agent according to GCM: 
    exp_df$a_choices <- gcm_agent(
      w = w, c = c, obs = exp_df[c("eyes", "legs", "spots", "arms", "color")], 
      cat_list = exp_df$danger)
    # Define correct choices, cumulative scores and performance: 
    exp_df$correct <- ifelse(exp_df$a_choices == exp_df$danger, 1, 0) 
    exp_df$cumulative <- cumsum(ifelse(exp_df$correct == 1, 1, -1))   
    exp_df$performance <- cumsum(exp_df$correct) / seq_along(exp_df$correct)
    if (i == 1){
      sim_data <- exp_df
    }
    else{
      sim_data <- rbind(sim_data, exp_df)
    }
  }
  cat("Finished simulating", n_agents/length(weights), "agents with w of", w, "\n")
}
```
## Plot Agents 

### Plot Performance and Cumulative Score of Simulation
```{r}
mean_sim <- sim_data %>% 
  group_by(w, trial) %>%   
  summarise(mean_cum = mean(cumulative), mean_perf = mean(performance))

ggplot()+
  geom_line(data = sim_data, aes(x = trial, y = cumulative),color = "grey")+
  geom_line(data = mean_sim, aes(x = trial, y = mean_cum, color = w), show.legend = F)+
  scale_color_viridis_d()+
  ggtitle("Simulation Cumulative Score (c: 1.5)")+
  facet_wrap(~w, labeller = label_both)+
  theme_minimal()
ggsave(filename = "figs/sim_cumulative.png")

ggplot()+
  geom_line(data = sim_data, aes(x = trial, y = performance), color = "grey")+
  geom_line(data = mean_sim, aes(x = trial, y = mean_perf, color = w), show.legend = F)+
  scale_color_viridis_d()+
  ggtitle("Simulation Performance (c: 1.5)")+
  facet_wrap(~w, labeller = label_both)+
  theme_minimal()
ggsave(filename = "figs/sim_performance.png")
```

### Percentage of Correct Categorisation per Stimuli Plot 
```{r}
## calculate percent correct categorisations
sim_data_sum <- pivot_wider(sim_data[c("w", "stimuli", "correct", "danger", "cycle")], 
                            names_from = correct, values_from = correct, values_fn = length, 
                            values_fill = 0)
colnames(sim_data_sum) <- c("w", "stimuli", "danger", "cycle", "wrong", "correct")
sim_data_sum <- sim_data_sum %>% 
  group_by(w, cycle, stimuli) %>% 
  mutate(perc_corr = (correct/(wrong+correct))*100) %>% 
  mutate_at( "cycle", as.factor)

ggplot(sim_data_sum) +
  geom_point(aes(x = cycle, y = stimuli,  size = perc_corr, fill = perc_corr), 
               alpha=0.7, shape = 21, position = position_jitter(width = 0.25)) + 
  scale_fill_viridis_c(breaks = c(30, 50, 70, 90), guide = "legend", name="% Correct")+ 
  scale_size_continuous(range = c(.1, 8),  name="% Correct", 
                        breaks = c(30, 50, 70, 90), guide = "legend")+
  theme_minimal()+ 
  facet_grid(row = vars(danger), col = vars(w), scales = "free_y", 
             space = "free_y", labeller = label_both)+
  theme(panel.grid = element_blank(), 
        panel.spacing = unit(1.5, "lines"))+ 
  coord_cartesian(clip = "off")
```





## Save in STAN-friendly Format
```{r}
# unique stimuli and category df with stimuli ids : 
stim_df <- unique(sim_data[c("danger", "eyes", "legs", "spots", "arms", "color")])
stim_df$s_id <- seq(1:nrow(stim_df))
sim_data <- left_join(sim_data, stim_df, #combine stim_df with ids and simulated df 
                      by = join_by("danger", "eyes", "legs", "spots", "arms", "color")) 

# observation matrix [nagents, ntrials] each value is the id for the observed stimuli
obs <- as.matrix(pivot_wider(sim_data[c("s_id", "agent_id", "trial")],
                             names_from = trial, values_from = s_id)[-1])
# choice matrix [nagents, ntrials] each value is the category assigned to the observed stimuli
choice <- as.matrix(pivot_wider(sim_data[c("a_choices", "agent_id", "trial")],
                             names_from = trial, values_from = a_choices)[-1])
# stimuli matrix [nstim, nfeatures] 
stimuli <- as.matrix(stim_df[c("eyes", "legs", "spots", "arms", "color")])

sim_data_ls <- list(
  nagents = n_agents,                # number of individual agents
  n_trials = 96,                     # number of trials for each agent 
  nfeatures = 5,                     # number of features for stimuli
  nstim = 32,                        # number of unique stimuli
  b = 0.5,                           # bias for selecting one cat over the other
  stimuli = stimuli,                 # matrix of [nstim, nfeatures] indices: s_id
  obs = obs,                         # matrix[nagents, ntrials] with obs
  cat_danger = stim_df$danger,       # array of cats with indices as s_id
  choice = choice,                   # matrix with choices [nagents, ntrials]
  w_prior_values = c(1, 1, 1, 1, 1), # priors for w
  c_prior_values = c(0, 1)           # priors for c
)

```


# STAN MODEL

```{r}
stan_gcm <- "
data{
  int<lower=1> nagents;                                   // number of agents
  int<lower=1> n_trials;                                  // number of trials 
  int<lower=1> nfeatures;                                 // number of features in each stimuli
  int<lower=1> nstim;                                     // number of unique stimuli
  real<lower=0, upper=1> b;                               // bias for selecting cat 'danger' (1)
  array[nstim, nfeatures] int<lower=0, upper=1> stimuli;  // features of each stimuli
  array[nagents, n_trials] int<lower=1, upper=32> obs;    // observations (s_id) for each agent+trial
  vector<lower=0, upper=1>[nstim] cat_danger;             // vector of categories for each unique stimuli
  array[nagents, n_trials] int<lower=0, upper=1> choice;  // agent-choice on each trial 
  
  // priors
  vector[nfeatures] w_prior_values;                       // priors for param w
  array[2] real c_prior_values;                           // mean and sd for logit-normal dist (param c)
}
parameters{
  row_stochastic_matrix[nagents, nfeatures] w;            // w param (each row is a simplex)
  vector[nagents] logit_c;                                // c param for each agent
}

transformed parameters{

  // Parameter c: inverse logit transformed 
  vector <lower=0, upper=2>[nagents] c = inv_logit(logit_c)*2;
  
  // Parameter r (probability of response = 1 ) 
  array[nagents, n_trials] real<lower=0.0001, upper=0.9999> r;
  array[nagents, n_trials] real rr; 

  for (a in 1:nagents){                     // Loop through each agent 
  	for (i in 1:n_trials){                  // Loop through each trial
  	
  		vector[(i-1)]  exemp_similarities;    // Vector of similarities between current obs and previous obs
  		
  		for (e in 1:(i-1)){                   // Loop through previous aliens
  			array[nfeatures] real tmp_distance; // Array of distances for each feature for previous alien e
  			
  			for (j in 1:nfeatures){             // Loop through each feature in alien e and calculate distances
  			  tmp_distance[j] = w[a,j]*abs(stimuli[obs[a, e], j] -  stimuli[obs[a, i], j] );
  			}
  			
  			// Calculate similarity between current alien and alien e:
  			exemp_similarities[e] = exp(-c[a] * sum(tmp_distance));
  		}
  		
  	  // If first trial or category is new, make r random: 	
      if (i == 1 || sum(cat_danger[obs[a, 1:(i-1)]]) == 0 || sum(cat_danger[obs[a, 1:(i-1)]]) == (i-1)){
        r[a, i] = 0.5;
      }
      
      // Otherwise calculate summed similarities (per category):
      else{
        array[2] real similarities; 
        
        vector[(i-1)] ind_danger = cat_danger[obs[a, 1:(i-1)]];      // vector of inds of dangerous stim
        vector[(i-1)] ind_safe = abs(cat_danger[obs[a, 1:(i-1)]]-1); // vector of inds of safe stim

        
        similarities[1] = sum(ind_danger.*exemp_similarities);       // sum of cat = 1
        similarities[2] = sum(ind_safe.*exemp_similarities);         // sum of cat = 0

        // Calculate probability of choosing 1:
        rr[a, i] = (b*similarities[1]) / (b*similarities[1] + (1-b)*similarities[2]);

        // (make sampling work)
        if (rr[a, i] > 0.9999){
          r[a, i] = 0.9999;
        } else if (rr[a, i] < 0.0001){
            r[a, i] = 0.0001;
        } else if (rr[a, i] > 0.0001 && rr[a, i] < 0.9999){
            r[a, i] = rr[a, i];
        } else{
            r[a, i] = 0.5;
        }
      }
  	}
  }
}

model {
  // Priors
  for (a in 1:nagents){
    target += dirichlet_lpdf(w[a,] | w_prior_values);
    target += normal_lpdf(logit_c[a] | c_prior_values[1], c_prior_values[2]);
  }
  // Liklihood
  
  for (a in 1:nagents){
    target += bernoulli_lpmf(choice[a, ] | r[a, ]);
  }
}

"
# Write the model to a file
write_stan_file(
  stan_gcm,
  dir = "stan/",
  basename = "gcm.stan"
)

```


# FIT & CHECK STAN ON SIMULATED DATA 

## Fit the STAN Model (Simulated Data)
```{r}
#if (!dir.exists("data")) dir.create("data")
#save(gcm_sum, draws_df_gcm, sim_data_ls, file = "data/sim_data_fit.RData")

file_gcm <- file.path("stan/gcm.stan")
mod_gcm <- cmdstan_model(file_gcm, 
                         cpp_options = list(stan_threads = TRUE))

samples_gcm <- mod_gcm$sample(
  data = sim_data_ls,
  seed = 10,
  refresh = 100,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 1,
  iter_warmup = 500,
  iter_sampling = 1000,
  max_treedepth = 20,
  adapt_delta = 0.99
)


gcm_sum <- samples_gcm$summary()
draws_df_gcm <- as_draws_df(samples_gcm$draws())
```

## Save Data
```{r}
## Save gcm_sum and draws_df_gcm as RData
save(gcm_sum, draws_df_gcm, sim_data_ls, file = "data/sim_data_fit.RData")
## Load data 
load("data/sim_data_fit.RData")
```


## Model Quality Checks (Simulated Data)
traceplots, posterior/prior checks

```{r}
#Extract draws 
#Converts CmdStan object into a usable dataframe for plotting and extraction
draws <- as_draws_df(samples_gcm)
#Extract all attention weights w[a,f]
w_cols <- grep("^w\\[", colnames(draws), value = TRUE)

##Trace Plots
draws_w_all <- draws %>%
  select(.iteration, .chain, all_of(w_cols)) %>%
  pivot_longer(cols = starts_with("w["), names_to = "param", values_to = "value") %>%
  separate(param, into = c("w", "agent", "feature"), sep = "\\[|,|\\]", convert = TRUE) %>%
  select(-w)

#Average over agents for each iteration/feature
avg_trace <- draws_w_all %>%
  group_by(.iteration, .chain, feature) %>%
  summarise(mean_value = mean(value), .groups = "drop")

#Plot traceplots of average attention per feature
ggplot(avg_trace, aes(x = .iteration, y = mean_value, group = .chain, color = .chain)) +
  geom_line(alpha = 0.6) +
  facet_wrap(~ feature, scales = "free_y", ncol = 1,
             labeller = labeller(feature = c(`1` = "Eyes", `2` = "Legs", `3` = "Spots", `4` = "Arms", `5` = "Color"))) +
  labs(title = "Markov Chains of Average Attention Across Agents per Feature",
       x = "Iteration", y = "Average Attention Weight") +
  theme_classic(base_size = 12) +
  theme(legend.position = "none")
ggsave(filename = "figs/trace_feat.png", width = 7, height = 5, dpi = 300)

# Define the parameters of interestbbecause other is average, I wanted to view more specific traces to verify convergence
params_trace <- c("c[1]", "c[10]", "c[20]", "c[30]",
                  "w[1,1]", "w[10,1]", "w[20,1]", "w[30,1]")

# Create long-format dataframe for trace plotting
draws_long <- draws %>%
  select(.iteration, .chain, all_of(params_trace)) %>%
  pivot_longer(cols = -c(.iteration, .chain),
               names_to = "parameter", values_to = "value")


##Plot using facet_wrap
ggplot(draws_long, aes(x = .iteration, y = value, group = .chain, color = .chain)) +
  geom_line(alpha = 0.5) +
  facet_wrap(~ parameter, scales = "free_y", ncol = 2) +
  labs(title = "Traceplots for Selected c and w Parameters",
       x = "Iteration", y = "Value") +
  theme_classic()
ggsave(filename = "figs/traceplots.png", width = 5, height = 7, dpi = 300)

#Extract parameter columns
#est_c: posterior mean sensitivity per agent
#est_w1: posterior mean attention to feature 1 per agent
#This reduces each agent's posterior distribution to a point estimate
c_cols <- grep("^c\\[", colnames(draws), value = TRUE)
w1_cols <- grep("^w\\[\\d+,1\\]$", colnames(draws), value = TRUE)

avg_w_posterior <- draws_w_all %>%
  group_by(.iteration) %>%
  summarise(value = mean(value), .groups = "drop")
prior_samples <- rdirichlet(nrow(avg_w_posterior) * 30, rep(1, 5))
# Generate the same number of prior samples as posterior
n_samples <- nrow(avg_w_posterior)
prior_matrix <- rdirichlet(n_samples, rep(1, 5))  # one sample per row
avg_w_prior <- rowMeans(prior_matrix)             # average across 5 features

plot_df <- bind_rows(
  tibble(value = avg_w_posterior$value, source = "Posterior"),
  tibble(value = avg_w_prior, source = "Prior")
)

ggplot(plot_df, aes(x = value, fill = source)) +
  geom_histogram(alpha = 0.4, position = "identity", bins = 30) +
  geom_vline(xintercept = 0.2, linetype = "dashed") +
  labs(title = "Posterior vs Prior (Average Attention Across Features)",
       x = "Average Attention Weight", y = "Count") +
  scale_fill_manual(values = c("Posterior" = "lightblue", "Prior" = "pink")) +
  theme_bw()
ggsave(filename = "figs/hist_avg_w.png", width = 7, height = 5, dpi = 300)

#Compute posterior means
#summarise full posterior into mean for modelling
agent_estimates <- tibble(
  agent = 1:length(c_cols),
  est_c = colMeans(draws[, c_cols]),
  est_w1 = colMeans(draws[, w1_cols])
)

##Histogram of c (sensitivity) across agents vs prior
#posterior blue and prior pink
draws_c_all <- draws %>%
  select(all_of(c_cols)) %>%
  pivot_longer(cols = everything(), names_to = "param", values_to = "value") %>%
  mutate(c_prior = rnorm(n(), 1, 1))

ggplot(draws_c_all) +
  geom_histogram(aes(x = value), fill = "lightblue", alpha = 0.6, bins = 30) +
  geom_histogram(aes(x = c_prior), fill = "pink", alpha = 0.3, bins = 30) +
  geom_vline(xintercept = 1.5, linetype = "dashed", color = "black") +
  labs(title = "Posterior vs Prior (All agents): Sensitivity Parameter", x = "c", y = "Count") +
  theme_bw()
ggsave(filename = "figs/hist1.png", width = 7, height = 5, dpi = 300)

##Histogram of feature 1 across agents vs prior
draws_w1_all <- draws %>%
  select(all_of(w1_cols)) %>%
  pivot_longer(cols = everything(), names_to = "param", values_to = "value") %>%
  mutate(w_prior = rdirichlet(n(), rep(1, 5))[,1])

ggplot(draws_w1_all) +
  geom_histogram(aes(x = value), fill = "lightblue", alpha = 0.6, bins = 30) +
  geom_histogram(aes(x = w_prior), fill = "pink", alpha = 0.3, bins = 30) +
  geom_vline(xintercept = 0.2, linetype = "dashed", color = "black") +
  labs(title = "Posterior vs Prior (All agents): Attention Weights", x = "w[1]", y = "Count") +
  theme_bw()
ggsave(filename = "figs/hist2.png", width = 7, height = 5, dpi = 300)

##Scatter of true vs estimated feature 1
ggplot(agent_estimates, aes(x = true_w1, y = est_w1)) +
  geom_jitter(width = 0.01, height = 0.01, size = 2, alpha = 0.7) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "True vs Estimated Attention to Feature 1",
       x = "True w[1]", y = "Estimated w[1]") +
  theme_minimal()
ggsave(filename = "figs/scatterT_Est.png", width = 7, height = 5, dpi = 300)

##Scatter of estimated c per agent
ggplot(agent_estimates, aes(x = agent, y = est_c)) +
  geom_point(size = 2) +
  geom_hline(yintercept = 1.5, linetype = "dashed") +
  labs(title = "Estimated c per Agent", y = "Estimated c", x = "Agent ID") +
  theme_classic()
ggsave(filename = "figs/estSim.png", width = 7, height = 5, dpi = 300)

```

#Across Features Attentional Weights

```{r}
#Compute posterior means for each agent-feature pair
agent_feature_estimates <- draws_w_all %>%
  group_by(agent, feature) %>%
  summarise(est_w = mean(value), .groups = "drop")

#Map feature numbers to labels
feature_labels <- c("Eyes", "Legs", "Spots", "Arms", "Color")

#Add labels
agent_feature_estimates <- agent_feature_estimates %>%
  mutate(feature_label = factor(feature_labels[feature], levels = feature_labels))

#Density plot of per-feature attention distributions
ggplot(agent_feature_estimates, aes(x = est_w, fill = feature_label)) +
  geom_density(alpha = 0.4) +
  labs(title = "Distribution of Attention Weights Across Features",
       x = "Estimated Attention Weight", y = "Density", fill = "Feature") +
  theme_minimal()
ggsave(filename = "figs/across_feat.png", width = 7, height = 5, dpi = 300)


```
# FIT & CHECK STAN ON EMPERICAL DATA

## Fit the STAN Model (Emperical Data)
```{r}

```

## Model Quality Checks (Emperical Data)
```{r}

```







