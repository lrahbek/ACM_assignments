---
title: "Assignment 3"
output: html_document
date: "2025-03-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Set up
```{r}
pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes, rstan, 
               ggplot2, dplyr, patchwork)
```


# FUNCTIONS

## Feedback Function
Function generates a feedback based on the 'first rating' and returns the group rating (first_rating+(feedback)). Feedback values can be between -3 and 3 (as in the original experiment). The feedback is sampled from from the list of values so the group rating does not lie outside the possible rating range (0-7). 
```{r}
feedback_fun <- function(first_rating){
  feedback_vals <- c(seq(-3, 3))
  feedback_lim <- feedback_vals[feedback_vals+first_rating>-1 & feedback_vals+first_rating<8] # possible feedback for current given first_rating (avoding group ratings below 0 and above 7)
  feedback <- sample(feedback_lim, 1)
  group_rating <- first_rating + (feedback)
  return(group_rating)
}
```


## Function for Simulating First- and Group Ratings
Simulates ratings for one agent for given amount of stimuli (trials)
```{r}
simulating_ratings <- function(n_stim, max_rating){
  ratings <- list(first_rating = rep(NA, n_stim), 
                  group_rating = rep(NA, n_stim))
  for (i in 1:n_stim){
    # simulating the first rating based on a randomly sampled probability value
    prob <- runif(1, min = 0.001, max = 0.999) 
    ratings$first_rating[i] <- rbinom(1, max_rating, prob) #rbinom(1,(max_rating-1),prob)+1
    # simulating the group rating, based on the value of the first rating
    ratings$group_rating[i] <- feedback_fun(ratings$first_rating[i])
  }
  return(ratings)
}
```

## Beta Binomial Agent - Simple Model
Simulating second_rating!
```{r}
betaBinomialModel_simple <- function(alpha_prior, beta_prior, first_ratings, 
                                     group_ratings, max_rating, n_stim) {
  #list of values 
  ratings2 <- list(second_rating = rep(NA, n_stim), 
                   expected_rate = rep(NA, n_stim), 
                   alpha_post = rep(NA, n_stim), 
                   beta_post = rep(NA, n_stim))
  #for each stimuli: 
  for (i in 1:n_stim){
    neg_first_rating <- max_rating - first_ratings[i] #the 'points' not given 
    neg_group_rating <- max_rating - group_ratings[i] #
    # calculate posterior beta and alpha values
    ratings2$alpha_post[i] <- alpha_prior + first_ratings[i] + group_ratings[i]
    ratings2$beta_post[i] <- beta_prior + neg_first_rating + neg_group_rating    
    # calculated second rating based on first and group rating
    ratings2$expected_rate[i] <- ratings2$alpha_post[i] /(
                                        ratings2$alpha_post[i] + ratings2$beta_post[i])  
    ratings2$second_rating[i] <- rbinom(1, 7, ratings2$expected_rate[i]) #+1
  }
  return(ratings2)
}
```

## Beta Binomial Agent - Weighted Model
```{r}
betaBinomialModel_weighted <- function(alpha_prior, beta_prior, first_ratings,
                                       group_ratings, max_rating, n_stim, 
                                       scaling_factor, weight_ratio) {
  #list of values 
  ratings2 <- list(second_rating = rep(NA, n_stim), 
                   expected_rate = rep(NA, n_stim), 
                   alpha_post = rep(NA, n_stim), 
                   beta_post = rep(NA, n_stim))
  weight_first <- scaling_factor * weight_ratio / (1+ weight_ratio)
  weight_group <- scaling_factor / (1 + weight_ratio)
  #for each stimuli: 
  for (i in 1:n_stim){
    neg_first_rating <- max_rating - first_ratings[i]
    neg_group_rating <- max_rating - group_ratings[i]
    #calculate weighted information 
    first_rating_w <- first_ratings[i] * weight_first
    group_rating_w <- group_ratings[i] * weight_group
    neg_first_rating_w <- neg_first_rating * weight_first
    neg_group_rating_w <- neg_group_rating * weight_group
    # calculate posterior beta and alpha values
    ratings2$alpha_post[i] <- alpha_prior + first_rating_w + group_rating_w
    ratings2$beta_post[i] <- beta_prior + neg_first_rating_w + neg_group_rating_w      
    # calculated second rating based on first and group rating
    ratings2$expected_rate[i] <- ratings2$alpha_post[i] /(
                                        ratings2$alpha_post[i] + ratings2$beta_post[i])  
    ratings2$second_rating[i] <- rbinom(1, 7, ratings2$expected_rate[i]) #+1
  }
  return(ratings2)
}

```


# SIMULATE DATA
Simulate data for both simple and weighted model
## Parameters for Both Models
```{r}
n_stim <- 153 # number of stimuli (faces)
max_rating <- 7 # maximum possible rating
n_agents <- 40 # number of agents
n_populations <- 1 # number of populations
```


## Simple Simulation
Alpha prior = 1
Beta prior = 1
```{r}
# Arrays for first and group ratings 
first_rating_s <- array(NA, dim = c(n_agents, n_stim))
group_rating_s <- array(NA, dim = c(n_agents, n_stim))

# Arrays for second rating, expected rate, alpha and beta posteriors
second_rating_s <- array(NA, dim = c(n_agents, n_stim))
expected_rate_s <- array(NA, dim = c(n_agents, n_stim))
alpha_post_s <- array(NA, dim = c(n_agents, n_stim))
beta_post_s <- array(NA, dim = c(n_agents, n_stim))

#loop through each agent
for (a in 1:n_agents){
  ratings <-  simulating_ratings(n_stim, max_rating) 
  first_rating_s[a,] <- ratings$first_rating
  group_rating_s[a,] <- ratings$group_rating
    
  second_ratings <- betaBinomialModel_simple(alpha_prior = 1, beta_prior = 1,
                                             first_rating = first_rating_s[a,],
                                             group_rating = group_rating_s[a,],
                                             max_rating, n_stim)
  second_rating_s[a, ] <- second_ratings$second_rating
  expected_rate_s[a,] <- second_ratings$expected_rate
  alpha_post_s[a,] <- second_ratings$alpha_post
  beta_post_s[a,]<- second_ratings$beta_post
}

#save data in STAN type of list 
data_simple <- list(n_stim = n_stim, 
                    n_agents = n_agents, 
                    max_rating = max_rating,
                    second_rating = second_rating_s,
                    first_rating = first_rating_s, 
                    group_rating = group_rating_s)
rm(first_rating_s, second_rating_s, group_rating_s, expected_rate_s, 
   alpha_post_s, beta_post_s, second_ratings, ratings) # clean up
```

## Weighted simulation
Parameters are defined in log-scale 
scaling factor mu = 1.5  - exp(1.5) = 4.48
scaling factor sigma = 0.3  - exp(0.3) = 1.34
weight ratio mu = 1 - exp(1) = 2.72
weight ratio sigma = 0.5 - exp(0.5) = 1.65
  
```{r}
# Arrays for first and group ratings 
first_rating_w <- array(NA, dim = c(n_populations, n_agents, n_stim))
group_rating_w <- array(NA, dim = c(n_populations, n_agents, n_stim))

# Arrays for second rating, expected rate, alpha and beta posteriors
second_rating_w <- array(NA, dim = c(n_populations, n_agents, n_stim))
expected_rate_w <- array(NA, dim = c(n_populations, n_agents, n_stim))
alpha_post_w <- array(NA, dim = c(n_populations, n_agents, n_stim))
beta_post_w <- array(NA, dim = c(n_populations, n_agents, n_stim))

# Arrays for true scaling_weight and weight_ratio values
log_scaling_weight <- array(NA, dim = c(n_populations, n_agents))
log_weight_ratio <- array(NA, dim = c(n_populations, n_agents))

scaling_factor <- array(NA, dim = c(n_populations, n_agents))
weight_ratio <- array(NA, dim = c(n_populations, n_agents))

weight <- list(scaling_weight_mu = rep(NA, n_populations), 
               scaling_weight_sigma = rep(NA, n_populations), 
               weight_ratio_mu = rep(NA, n_populations),
               weight_ratio_sigma = rep(NA, n_populations))


# loop through the population
for (p in 1:n_populations){
  
  # Define population parameters for weighted model
  scaling_weight_mu <- 1.5   # Mean scaling factor (log-scale) exp(1.5) = 4.48
  scaling_weight_sigma <- 0.3     # SD of scaling factor (log-scale) exp(0.3) = 1.34
  weight_ratio_mu <- 1     # Mean weight ratio (log-scale, 0 = equal weights) exp(1) = 2.72
  weight_ratio_sigma <- 0.5       # SD of weight ratio (log-scale) exp(0.5) = 1.65
  
  #loop through each agent
  for (a in 1:n_agents){
    # draw weight values from population distriutions
    log_scaling_weight[p, a] <- rnorm(1, mean = scaling_weight_mu, sd = scaling_weight_sigma)
    scaling_factor[p, a] = exp(log_scaling_weight[p, a])
    log_weight_ratio[p, a] <- rnorm(1, mean = weight_ratio_mu, sd = weight_ratio_sigma)
    weight_ratio[p, a] = exp(log_weight_ratio[p, a])
    
    # simulate first and group ratings
    ratings <-  simulating_ratings(n_stim, max_rating)
    first_rating_w[p, a,] <- ratings$first_rating
    group_rating_w[p, a,] <- ratings$group_rating
    
    # extract second ratings based on first and group ratings 
    second_ratings <- betaBinomialModel_weighted(alpha_prior = 1, beta_prior = 1,
                                                 first_rating = first_rating_w[p, a,], 
                                                 group_rating = group_rating_w[p, a,],
                                                 max_rating, n_stim, 
                                                 scaling_factor = scaling_factor[p, a], 
                                                 weight_ratio = weight_ratio[p, a])
    
    second_rating_w[p, a, ] <- second_ratings$second_rating
    expected_rate_w[p, a,] <- second_ratings$expected_rate
    alpha_post_w[p, a,] <- second_ratings$alpha_post
    beta_post_w[p, a,]<- second_ratings$beta_post
  }
  weight$scaling_weight_mu[p] <- scaling_weight_mu
  weight$scaling_weight_sigma[p] <- scaling_weight_sigma
  weight$weight_ratio_mu[p] <- weight_ratio_mu
  weight$weight_ratio_sigma[p] <- weight_ratio_sigma
}

#save data in STAN type of list 

data_weighted <- list(n_stim = n_stim, 
                      n_agents = n_agents, 
                      n_populations = n_populations,
                      max_rating = max_rating,
                      second_rating = second_rating_w,
                      first_rating = first_rating_w, 
                      group_rating = group_rating_w)

rm(first_rating_w, second_rating_w, group_rating_w, expected_rate_w, 
   alpha_post_w, beta_post_w, second_ratings, ratings, weight_ratio_mu, 
   weight_ratio_sigma, scaling_weight_mu, scaling_weight_sigma) # clean up
```


# STAN MODELS 
## STAN MODEL of the Simple Agents
```{r}
stan_model_simple <- "
data {
  int <lower=1> n_stim;
  int <lower=1> n_agents;
  int <lower=1> max_rating;
  array[n_agents, n_stim] int <lower=0, upper=7> first_rating;
  array[n_agents, n_stim] int <lower=0, upper=7> second_rating;
  array[n_agents, n_stim] int <lower=0, upper=7> group_rating;
}
parameters{
  real <lower=0, upper=10> alpha_prior;
  real <lower=0, upper=10> beta_prior;
}
model {
  //priors
  target += uniform_lpdf(alpha_prior | 1, 3);
  target += uniform_lpdf(beta_prior | 1, 3);  
  
  for (i in 1:n_agents) {
    for (j in 1:n_stim) {
      real alpha_post = alpha_prior + first_rating[i, j] + group_rating[i, j];
      real beta_post = beta_prior + (max_rating - first_rating[i, j]) + (max_rating - group_rating[i, j]);
      
      // model the second rating
      target += beta_binomial_lpmf(second_rating[i, j] | 7, alpha_post, beta_post);
    }
  }
}
generated quantities {

  array[n_agents, n_stim] real log_lik;
  array[n_agents, n_stim] int prior_pred_rating;
  array[n_agents, n_stim] int post_pred_rating;

  for (i in 1:n_agents) {
    for (j in 1:n_stim) {
      
      //generate prior predictions
      real alpha_prior_prior = uniform_rng(1, 3);
      real beta_prior_prior = uniform_rng(1, 3);
      
      real alpha_post_prior = alpha_prior_prior + first_rating[i, j] + group_rating[i, j];
      real beta_post_prior = beta_prior_prior + (max_rating - first_rating[i, j]) + (max_rating - group_rating[i, j]);
      
      prior_pred_rating[i, j] = beta_binomial_rng(7, alpha_post_prior, beta_post_prior);
      
      // posterior predictions 
      real alpha_post = alpha_prior + first_rating[i, j] + group_rating[i, j];
      real beta_post = beta_prior+(max_rating - first_rating[i, j]) + (max_rating - group_rating[i, j]);
      
      post_pred_rating[i, j] = beta_binomial_rng(7, alpha_post, beta_post);
      
      log_lik[i, j] = beta_binomial_lpmf(second_rating[i, j] | 7, alpha_post, beta_post);
    }
  }
}
"
write_stan_file(
  stan_model_simple, dir = "stan/", basename = "simple_agent.stan")
rm(stan_model_simple) #clean up
```

## STAN MODEL of the Weighted Agents
```{r}
stan_model_weighted <- "
data {
  int <lower=1> n_stim;
  int <lower=1> n_populations;
  int <lower=1> n_agents;
  int <lower=1> max_rating;
  array[n_populations, n_agents, n_stim] int <lower=0, upper=7> first_rating;
  array[n_populations, n_agents, n_stim] int <lower=0, upper=7> second_rating;
  array[n_populations, n_agents, n_stim] int <lower=0, upper=7> group_rating;
}
parameters {
  // population level params
  array[n_populations] real <lower=-6, upper=10> scaling_mu;
  array[n_populations] real <lower=0> scaling_sigma;
  array[n_populations] real <lower=-6, upper=6> weight_ratio_mu;
  array[n_populations] real <lower=0> weight_ratio_sigma;
  
  // Individual-level (random) effects
  array[n_populations, n_agents] real z_weight_ratio;              
  array[n_populations, n_agents] real z_scaling;                   
}
transformed parameters {
  // Individual-level parameters
  array[n_populations, n_agents] real <lower=0> weight_ratio;  
  array[n_populations, n_agents] real <lower=0> scaling_factor;     
  array[n_populations, n_agents] real <lower=0> weight_first;     
  array[n_populations, n_agents] real <lower=0> weight_group;      
  
  // Non-centered parameterization
  for (p in 1:n_populations){
    for (i in 1:n_agents){
      weight_ratio[p, i] = exp(weight_ratio_mu[p] + z_weight_ratio[p, i] * weight_ratio_sigma[p]);
      scaling_factor[p, i] = exp(scaling_mu[p] + z_scaling[p, i] * scaling_sigma[p]);
      
      weight_first[p, i] = scaling_factor[p, i] * weight_ratio[p, i] / (1 + weight_ratio[p, i]);
      weight_group[p, i] = scaling_factor[p, i]  / (1 + weight_ratio[p, i]);
    }
  }
}
model {
  for (p in 1:n_populations){
    // Population level priors 
    target += normal_lpdf(weight_ratio_mu[p] | 0, 1);
    target += normal_lpdf(scaling_mu[p] | 0, 1);
    target += exponential_lpdf(weight_ratio_sigma[p] | 2);        
    target += exponential_lpdf(scaling_sigma[p] | 2);    
    
    for (i in 1:n_agents){
      // Agent level priors 
      target += std_normal_lpdf(z_weight_ratio[p, i]);
      target += std_normal_lpdf(z_scaling[p, i]);
    }
  }
  // Likelihood
  for (p in 1:n_populations){
    for (i in 1:n_agents){
      real w_first = weight_first[p, i];
      real w_group = weight_group[p, i];
      
      for (j in 1:n_stim){
        real first_rating_w = first_rating[p, i, j] * w_first;
        real group_rating_w = group_rating[p, i, j] * w_group;
        
        real neg_first_rating_w = (max_rating - first_rating[p, i, j]) * w_first;
        real neg_group_rating_w = (max_rating - group_rating[p, i, j]) * w_group;
        
        real alpha_post = 1 + first_rating_w + group_rating_w;
        real beta_post = 1 + neg_first_rating_w + neg_group_rating_w;
        
        target += beta_binomial_lpmf(second_rating[p, i, j] | 7, alpha_post, beta_post);
      }
    }
  }
}
generated quantities {
  
  // Arrays for converted population parameters 
  array[n_populations] real population_ratio;
  array[n_populations] real population_scaling;
  array[n_populations] real population_weight_first;
  array[n_populations] real population_weight_group;

  // Arrays for log liklihood, prior and posterior predictive second ratings 
  array[n_populations, n_agents, n_stim] real log_lik;  // Log likelihood for model comparison
  array[n_populations, n_agents, n_stim] int prior_pred_rating; // prior predictive second ratings
  array[n_populations, n_agents, n_stim] int post_pred_rating; // posterior predictiove second ratings
  
  // Generate prior and posterior predictive ratings: 
  
  for (p in 1:n_populations){
    // Calculate converted parameters: 
    population_ratio[p] = exp(weight_ratio_mu[p]);
    population_scaling[p] = exp(scaling_mu[p]);
    population_weight_first[p] = population_scaling[p] * population_ratio[p] / (1 + population_ratio[p]);
    population_weight_group[p] = population_scaling[p] / (1 + population_ratio[p]);
    
    // Generate population level values for prior predictive checks
    real weight_ratio_mu_prior = normal_rng(0, 1);
    real weight_ratio_sigma_prior = exponential_rng(2);
    real scaling_mu_prior = normal_rng(0,1);
    real scaling_sigma_prior = exponential_rng(2);

    for (i in 1:n_agents){
      // Define weights for first and group information for posterior predictive checks
      real w_first = weight_first[p, i];
      real w_group = weight_group[p, i];
      
      // Generate individual level variance values for prior predictive checks
      real z_weight_ratio_prior = std_normal_rng();
      real z_scaling_prior = std_normal_rng();
      
      // Calculate weight ratio and scaling factor for prior predictive checks
      real weight_ratio_prior= exp(weight_ratio_mu_prior + z_weight_ratio_prior * weight_ratio_sigma_prior);
      real scaling_factor_prior = exp(scaling_mu_prior + z_scaling_prior * scaling_sigma_prior);
      
      // Calculate weights for first and group information for prior predivtive checks 
      real w_first_prior = scaling_factor_prior * weight_ratio_prior / (1 + weight_ratio_prior);
      real w_group_prior = scaling_factor_prior / (1 + weight_ratio_prior);
      
      for (j in 1:n_stim){
        // calculate weighted information: posterior predictive checks  
        real first_rating_w = first_rating[p, i, j] * w_first;
        real group_rating_w = group_rating[p, i, j] * w_group;
        real neg_first_rating_w = (max_rating - first_rating[p, i, j]) * w_first;
        real neg_group_rating_w = (max_rating - group_rating[p, i, j]) * w_group;
        real alpha_post = 1 + first_rating_w + group_rating_w;
        real beta_post = 1 + neg_first_rating_w + neg_group_rating_w;
        
        // Posterior predictive second ratings 
        post_pred_rating[p, i, j] = beta_binomial_rng(7, alpha_post, beta_post);
        
        // Calculate log likelihood
        log_lik[p, i, j] = beta_binomial_lpmf(second_rating[p, i, j] | 7, alpha_post, beta_post); 
        
        // calculate weighted information: prior predictive checks  
        real first_rating_w_prior = first_rating[p, i, j] * w_first_prior;
        real group_rating_w_prior = group_rating[p, i, j] * w_group_prior;
        real neg_first_rating_w_prior = (max_rating - first_rating[p, i, j]) * w_first_prior;
        real neg_group_rating_w_prior = (max_rating - group_rating[p, i, j]) * w_group_prior;
        real alpha_post_prior = 1 + first_rating_w_prior + group_rating_w_prior;
        real beta_post_prior = 1 + neg_first_rating_w_prior + neg_group_rating_w_prior;
        
        // Posterior predictive second ratings 
        prior_pred_rating[p, i, j] = beta_binomial_rng(7, alpha_post_prior, beta_post_prior);
        
      }
    }
  }
}
"
write_stan_file(
  stan_model_weighted, dir = "stan/", basename = "weighted_agent.stan")
rm(stan_model_weighted) #clean up
```



# COMPILE AND FIT MODELS: SIMULATED DATA
## Compile and Fit Simple Model
```{r}
file_simple <- file.path("stan/simple_agent.stan")
mod_simple <- cmdstan_model(file_simple, 
                            cpp_options = list(stan_threads = TRUE))

samples_simple <- mod_simple$sample(
  data = data_simple,
  seed = 120,
  refresh = 500,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
simple_sum <- samples_simple$summary()
draws_df_simple <- as_draws_df(samples_simple$draws())
```
## Summary of Parameters of Interest for Simple Model
```{r}
simple_sum[simple_sum$variable == "alpha_prior" | simple_sum$variable == "beta_prior",]
```

## Compile and Fit Weighted Model
```{r}
file_weighted <- file.path("stan/weighted_agent.stan")
mod_weighted <- cmdstan_model(file_weighted, 
                              cpp_options = list(stan_threads = TRUE))

samples_weighted <- mod_weighted$sample(
  data = data_weighted,
  seed = 120,
  refresh = 500,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
weighted_sum <- samples_weighted$summary()
draws_df_weighted <- as_draws_df(samples_weighted$draws())
```

## Summary of Parameters of Interest for Weighted Model
```{r}
weighted_sum[weighted_sum$variable == "population_ratio[1]" |
            weighted_sum$variable == "population_scaling[1]"| 
            weighted_sum$variable == "weight_ratio_sigma[1]" |
            weighted_sum$variable == "scaling_sigma[1]" |
            weighted_sum$variable == "population_weight_first[1]" |
            weighted_sum$variable == "population_weight_group[1]",  ]

## true scaling sigma = 0.3 (exp(03)= 1.34
## true ratio sigma = 0.5 (exp(0.5) = 1.65)
## true scaling = 4.48, true ratio = 2.72
```


## Convergence Checks for Simple and Weighted Models 
```{r}
cat("Convergence checks for simple and weighted model \n")
simple_rhat_issues <- simple_sum %>%
  filter(rhat > 1.05) %>% nrow()
cat("Simple model parameters with Rhat > 1.05:", 
    simple_rhat_issues, "out of", nrow(simple_sum), "\n")

weighted_rhat_issues <- weighted_sum %>%
  filter(rhat > 1.05) %>% nrow()
cat("Weighted model parameters with Rhat > 1.05:", 
    weighted_rhat_issues, "out of", nrow(weighted_sum), "\n")
```


# MODEL QUALITY CHECKS
## Markov Chains - Simple Model 
```{r}
ggplot(draws_df_simple, aes(.iteration, alpha_prior, group = .chain, color = .chain)) +
  geom_line() +
  ggtitle("Simple Model: Alpha Prior")+
  theme_classic()
ggsave("figs/markov_chains_simple_alpha_prior.png")

ggplot(draws_df_simple, aes(.iteration, beta_prior, group = .chain, color = .chain)) +
  geom_line() +
  ggtitle("Simple Model: Beta Prior")+
  theme_classic()
ggsave("figs/markov_chains_simple_beta_prior.png")
```

## Markov Chains - Weighted Model
```{r}
draws_df_weighted %>%
  select(.iteration, .chain, starts_with("weight_ratio_mu[")) %>%
  pivot_longer(cols = starts_with("weight_ratio_mu["), names_to = "param", values_to = "value") %>%
  ggplot(aes(x = .iteration, y = value, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic() +
  labs(title = "Weighted Model: Weight Ratio Mu")
ggsave("figs/markov_chains_weighted_weight_ratio_mu.png")

draws_df_weighted %>%
  select(.iteration, .chain, starts_with("weight_ratio_sigma[")) %>%
  pivot_longer(cols = starts_with("weight_ratio_sigma["), names_to = "param", values_to = "value") %>%
  ggplot(aes(x = .iteration, y = value, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic() +
  labs(title = "Weighted Model: Weight Ratio Sigma")
ggsave("figs/markov_chains_weighted_weight_ratio_sigma.png")

draws_df_weighted %>%
  select(.iteration, .chain, starts_with("scaling_mu[")) %>%
  pivot_longer(cols = starts_with("scaling_mu["), names_to = "param", values_to = "value") %>%
  ggplot(aes(x = .iteration, y = value, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic() +
  labs(title = "Weighted Model: Weight Scaling Mu")
ggsave("figs/markov_chains_weighted_scaling_mu.png")

draws_df_weighted %>%
  select(.iteration, .chain, starts_with("scaling_sigma[")) %>%
  pivot_longer(cols = starts_with("scaling_sigma["), names_to = "param", values_to = "value") %>%
  ggplot(aes(x = .iteration, y = value, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic() +
  labs(title = "Weighted Model: Weight Ratio Sigma")
ggsave("figs/markov_chains_weighted_scaling_sigma.png")
```

# FIT MODELS: REAL DATA
## Load Data and Format to List
as the weighted model is multilevel, and the simple model isnt explicitly (i.e. one has 3D arrays and one hasnt), two different data versions are made.
```{r}
# Load data
data <- read_csv("data/Simonsen_clean.csv", show_col_types = F,
         col_select = c("ID", "FaceID",
                        "FirstRating", "GroupRating", "SecondRating"))

# save data in STAN-friendly list 
data_real <- list(n_stim = length(unique(data$FaceID)), 
                  n_agents = length(unique(data$ID)), 
                  max_rating = 7,
                  second_rating = NA,
                  first_rating = NA, 
                  group_rating = NA)

length(unique(data$ID))
# make a data version for each of the models : simple
data_real_simple <- data_real
data_real_simple$second_rating <- array(data$SecondRating-1, 
                                 dim = c(data_real$n_agents, data_real$n_stim))
data_real_simple$first_rating <- array(data$FirstRating-1, 
                                 dim = c(data_real$n_agents, data_real$n_stim))
data_real_simple$group_rating <- array(data$GroupRating-1, 
                                 dim = c(data_real$n_agents, data_real$n_stim))

# make a data version for each of the models : weighted
data_real_weighted <- data_real
data_real_weighted$n_populations <- 1
data_real_weighted$second_rating <- array(
  data$SecondRating-1, dim = c(1, data_real$n_agents, data_real$n_stim))
data_real_weighted$first_rating <- array(
  data$FirstRating-1, dim = c(1, data_real$n_agents, data_real$n_stim))
data_real_weighted$group_rating <- array(
  data$GroupRating-1, dim = c(1, data_real$n_agents, data_real$n_stim))
rm(data_real) #clean up
```


## Fit Simple Model to Real Data & Summarise Parameters
```{r}
samples_simple_DR <- mod_simple$sample(
  data = data_real_simple,
  seed = 120,
  refresh = 500,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
simple_sum_DR <- samples_simple_DR$summary()
draws_simple_DR <- as_draws_df(samples_simple_DR$draws()) 
simple_sum_DR[simple_sum_DR$variable == "alpha_prior" | simple_sum_DR$variable == "beta_prior",]
```

## Fit Weighted Model to Real Data and Summarise Parameters
```{r}
samples_weighted_DR <- mod_weighted$sample(
  data = data_real_weighted,
  seed = 120,
  refresh = 500,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
weighted_sum_DR <- samples_weighted_DR$summary()
draws_weighted_DR <- as_draws_df(samples_weighted_DR$draws())

weighted_sum_DR[weighted_sum_DR$variable == "population_ratio[1]" |
                weighted_sum_DR$variable == "population_scaling[1]"| 
                weighted_sum_DR$variable == "weight_ratio_sigma[1]" |
                weighted_sum_DR$variable == "scaling_sigma[1]" |
                weighted_sum_DR$variable == "population_weight_first[1]" |
                weighted_sum_DR$variable == "population_weight_group[1]",  ]
```


## Convergence Checks for Simple and Weighted Models: REAL DATA
```{r}
cat("Convergence checks for simple and weighted model (fit to real data) \n")
simple_rhat_issues_DR <- simple_sum_DR %>%
  filter(rhat > 1.05) %>% nrow()
cat("Simple model parameters with Rhat > 1.05:", 
    simple_rhat_issues_DR, "out of", nrow(simple_sum_DR), "\n")

weighted_rhat_issues_DR <- weighted_sum_DR %>%
  filter(rhat > 1.05) %>% nrow()
cat("Weighted model parameters with Rhat > 1.05:", 
    weighted_rhat_issues_DR, "out of", nrow(weighted_sum_DR), "\n")
```


# MODEL COMPARISON
## Calculate: LOO-CV
```{r}
loo_simple <- samples_simple_DR$loo()
loo_weighted <- samples_weighted_DR$loo()
loo_comparison <- loo_compare(list(simple = loo_simple, weighted = loo_weighted))
print(loo_comparison)
```

## Calculate and Visualize model weights
```{r}
model_weights <- loo_model_weights(list(
  "Simple Integration" = loo_simple,
  "Weighted Integration" = loo_weighted))

# Print model weights
print(model_weights)

model_comp_data <- tibble(
  model = names(model_weights),
  weight = as.numeric(model_weights))

ggplot(model_comp_data, aes(x = model, y = weight, fill = model)) +
  geom_col() +
  geom_text(aes(label = scales::percent(weight, accuracy = 0.1)), 
            vjust = -0.5, size = 5) +
  labs(title = "Model Comparison Using LOO-CV",
       subtitle = "Higher weights indicate better predictive performance",
       x = NULL, y = "Model Weight") +
  scale_fill_viridis_d(begin = 0.1, end = 0.5)+
  #scale_fill_brewer(palette = "Accent") +
  theme_minimal() +   ylim(0,0.7)+
  theme(legend.position = "none")
ggsave("figs/model_weights.png")
```

## Compare Model for each Participant
```{r}
# Calculate pointwise ELPD values for each model
elpd_simple <- loo_simple$pointwise[, "elpd_loo"]
elpd_weighted <- loo_weighted$pointwise[, "elpd_loo"]
# Aggregate by agent
elpd_by_agent <- data %>% 
  select(ID) %>% 
  distinct() %>% 
  mutate(elpd_simple = NA_real_, elpd_weighted = NA_real_)

# Calculate ELPD sums by agent
for (i in 1:nrow(elpd_by_agent)) {
  agent <- elpd_by_agent$ID[i]
  # Find rows for this agent
  agent_rows <- which(data$ID == agent)
  # Sum ELPD values for this agent
  elpd_by_agent$elpd_simple[i] <- sum(elpd_simple[agent_rows])
  elpd_by_agent$elpd_weighted[i] <- sum(elpd_weighted[agent_rows])
}
# Calculate ELPD difference (positive = weighted model is better)
elpd_by_agent <- elpd_by_agent %>%
  mutate(elpd_diff = elpd_weighted - elpd_simple,
         better_model = ifelse(elpd_diff > 0, "Weighted", "Simple"))

```

## Vizualisation of Model Preference for each Participant
```{r}
ggplot(elpd_by_agent, aes(x = as.factor(ID), y = elpd_diff, color = elpd_diff)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Model Preference by Participant",
       subtitle = "Positive values favor the weighted model; negative values favor the simple model",
    x = "Participant ID",
    y = "ELPD Difference (Weighted - Simple)",
    color = "ELPD Difference") +
  scale_color_viridis_c()+
  guides(x = guide_axis(angle = 90))+
  theme_minimal()
ggsave("figs/ELPD_diff.png")
```






