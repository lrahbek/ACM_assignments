---
title: "Assignment 3"
output: html_document
date: "2025-03-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Set up
```{r}
pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes, rstan, 
               ggplot2, dplyr, patchwork)
```


# FUNCTIONS

## Feedback function
Function generates a feedback based on the 'first rating' and returns the group rating (first_rating+(feedback)). Feedback values can be between -3 and 3 (as in the original experiment). The feedback is sampled from from the list of values so the group rating does not lie outside the possible rating range (0-7). 
```{r}
feedback_fun <- function(first_rating){
  feedback_vals <- c(seq(-3, 3))
  feedback_lim <- feedback_vals[feedback_vals+first_rating>-1 & feedback_vals+first_rating<8] # possible feedback for current given first_rating (avoding group ratings below 0 and above 7)
  feedback <- sample(feedback_lim, 1)
  group_rating <- first_rating + (feedback)
  return(group_rating)
}
```


## Function for simulating first- and group-rating
Simulates ratings for one agent for given amount of stimuli (trials)
```{r}
simulating_ratings <- function(n_stim, max_rating){
  ratings <- list(first_rating = rep(NA, n_stim), 
                  group_rating = rep(NA, n_stim))
  for (i in 1:n_stim){
    # simulating the first rating based on a randomly sampled probability value
    prob <- runif(1, min = 0.001, max = 0.999) 
    ratings$first_rating[i] <- rbinom(1, max_rating, prob) #rbinom(1,(max_rating-1),prob)+1
    # simulating the group rating, based on the value of the first rating
    ratings$group_rating[i] <- feedback_fun(ratings$first_rating[i])
  }
  return(ratings)
}
```

## Beta Binomial Agent - simple model
Simulating second_rating!
```{r}
betaBinomialModel_simple <- function(alpha_prior, beta_prior, first_ratings, 
                                     group_ratings, max_rating, n_stim) {
  #list of values 
  ratings2 <- list(second_rating = rep(NA, n_stim), 
                   expected_rate = rep(NA, n_stim), 
                   alpha_post = rep(NA, n_stim), 
                   beta_post = rep(NA, n_stim))
  #for each stimuli: 
  for (i in 1:n_stim){
    neg_first_rating <- max_rating - first_ratings[i] #the 'points' not given 
    neg_group_rating <- max_rating - group_ratings[i] #
    # calculate posterior beta and alpha values
    ratings2$alpha_post[i] <- alpha_prior + first_ratings[i] + group_ratings[i]
    ratings2$beta_post[i] <- beta_prior + neg_first_rating + neg_group_rating    
    # calculated second rating based on first and group rating
    ratings2$expected_rate[i] <- ratings2$alpha_post[i] /(
                                        ratings2$alpha_post[i] + ratings2$beta_post[i])  
    ratings2$second_rating[i] <- rbinom(1, 7, ratings2$expected_rate[i]) #+1
  }
  return(ratings2)
}
```

## Beta Binomial Agent - weighted model
```{r}
betaBinomialModel_weighted <- function(alpha_prior, beta_prior, first_ratings,
                                       group_ratings, max_rating, n_stim, 
                                       scaling_factor, weight_ratio) {
  #list of values 
  ratings2 <- list(second_rating = rep(NA, n_stim), 
                   expected_rate = rep(NA, n_stim), 
                   alpha_post = rep(NA, n_stim), 
                   beta_post = rep(NA, n_stim))
  weight_first <- scaling_factor * weight_ratio / (1+ weight_ratio)
  weight_group <- scaling_factor / (1 + weight_ratio)
  #for each stimuli: 
  for (i in 1:n_stim){
    neg_first_rating <- max_rating - first_ratings[i]
    neg_group_rating <- max_rating - group_ratings[i]
    #calculate weighted information 
    first_rating_w <- first_ratings[i] * weight_first
    group_rating_w <- group_ratings[i] * weight_group
    neg_first_rating_w <- neg_first_rating * weight_first
    neg_group_rating_w <- neg_group_rating * weight_group
    # calculate posterior beta and alpha values
    ratings2$alpha_post[i] <- alpha_prior + first_rating_w + group_rating_w
    ratings2$beta_post[i] <- beta_prior + neg_first_rating_w + neg_group_rating_w      
    # calculated second rating based on first and group rating
    ratings2$expected_rate[i] <- ratings2$alpha_post[i] /(
                                        ratings2$alpha_post[i] + ratings2$beta_post[i])  
    ratings2$second_rating[i] <- rbinom(1, 7, ratings2$expected_rate[i]) #+1
  }
  return(ratings2)
}

```


# SIMULATE DATA
Simulate data for both simple and weighted model
## Parameters for both models
```{r}
n_stim <- 153 # number of stimuli (faces)
max_rating <- 7 # maximum possible rating
n_agents <- 40 # number of agents
n_populations <- 1 # number of populations
```


## Simple (non weighted) simulation
```{r}
# Arrays for first and group ratings 
first_rating_s <- array(NA, dim = c(n_agents, n_stim))
group_rating_s <- array(NA, dim = c(n_agents, n_stim))

# Arrays for second rating, expected rate, alpha and beta posteriors
second_rating_s <- array(NA, dim = c(n_agents, n_stim))
expected_rate_s <- array(NA, dim = c(n_agents, n_stim))
alpha_post_s <- array(NA, dim = c(n_agents, n_stim))
beta_post_s <- array(NA, dim = c(n_agents, n_stim))

#loop through each agent
for (a in 1:n_agents){
  ratings <-  simulating_ratings(n_stim, max_rating) 
  first_rating_s[a,] <- ratings$first_rating
  group_rating_s[a,] <- ratings$group_rating
    
  second_ratings <- betaBinomialModel_simple(alpha_prior = 1, beta_prior = 1,
                                             first_rating = first_rating_s[a,],
                                             group_rating = group_rating_s[a,],
                                             max_rating, n_stim)
  second_rating_s[a, ] <- second_ratings$second_rating
  expected_rate_s[a,] <- second_ratings$expected_rate
  alpha_post_s[a,] <- second_ratings$alpha_post
  beta_post_s[a,]<- second_ratings$beta_post
}

#save data in STAN type of list 
data_simple <- list(n_stim = n_stim, 
                    n_agents = n_agents, 
                    max_rating = max_rating,
                    second_rating = second_rating_s,
                    first_rating = first_rating_s, 
                    group_rating = group_rating_s)
rm(first_rating_s, second_rating_s, group_rating_s, expected_rate_s, 
   alpha_post_s, beta_post_s, second_ratings, ratings) # clean up
```

## Weighted simulation
```{r}
# Arrays for first and group ratings 
first_rating_w <- array(NA, dim = c(n_populations, n_agents, n_stim))
group_rating_w <- array(NA, dim = c(n_populations, n_agents, n_stim))

# Arrays for second rating, expected rate, alpha and beta posteriors
second_rating_w <- array(NA, dim = c(n_populations, n_agents, n_stim))
expected_rate_w <- array(NA, dim = c(n_populations, n_agents, n_stim))
alpha_post_w <- array(NA, dim = c(n_populations, n_agents, n_stim))
beta_post_w <- array(NA, dim = c(n_populations, n_agents, n_stim))

# Arrays for true scaling_weight and weight_ratio values
log_scaling_weight <- array(NA, dim = c(n_populations, n_agents))
log_weight_ratio <- array(NA, dim = c(n_populations, n_agents))

scaling_factor <- array(NA, dim = c(n_populations, n_agents))
weight_ratio <- array(NA, dim = c(n_populations, n_agents))

weight <- list(scaling_weight_mu = rep(NA, n_populations), 
               scaling_weight_sigma = rep(NA, n_populations), 
               weight_ratio_mu = rep(NA, n_populations),
               weight_ratio_sigma = rep(NA, n_populations))


# loop through the population
for (p in 1:n_populations){
  
  # Define population parameters for weighted model
  scaling_weight_mu <- 1.5   # Mean scaling factor (log-scale) exp(1.5) = 4.48
  scaling_weight_sigma <- 0.3     # SD of scaling factor (log-scale) exp(0.3) = 1.34
  weight_ratio_mu <- 1     # Mean weight ratio (log-scale, 0 = equal weights) exp(1) = 2.72
  weight_ratio_sigma <- 0.5       # SD of weight ratio (log-scale) exp(0.5) = 1.65
  
  #loop through each agent
  for (a in 1:n_agents){
    # draw weight values from population distriutions
    log_scaling_weight[p, a] <- rnorm(1, mean = scaling_weight_mu, sd = scaling_weight_sigma)
    scaling_factor[p, a] = exp(log_scaling_weight[p, a])
    log_weight_ratio[p, a] <- rnorm(1, mean = weight_ratio_mu, sd = weight_ratio_sigma)
    weight_ratio[p, a] = exp(log_weight_ratio[p, a])
    
    # simulate first and group ratings
    ratings <-  simulating_ratings(n_stim, max_rating)
    first_rating_w[p, a,] <- ratings$first_rating
    group_rating_w[p, a,] <- ratings$group_rating
    
    # extract second ratings based on first and group ratings 
    second_ratings <- betaBinomialModel_weighted(alpha_prior = 1, beta_prior = 1,
                                                 first_rating = first_rating_w[p, a,], 
                                                 group_rating = group_rating_w[p, a,],
                                                 max_rating, n_stim, 
                                                 scaling_factor = scaling_factor[p, a], 
                                                 weight_ratio = weight_ratio[p, a])
    
    second_rating_w[p, a, ] <- second_ratings$second_rating
    expected_rate_w[p, a,] <- second_ratings$expected_rate
    alpha_post_w[p, a,] <- second_ratings$alpha_post
    beta_post_w[p, a,]<- second_ratings$beta_post
  }
  weight$scaling_weight_mu[p] <- scaling_weight_mu
  weight$scaling_weight_sigma[p] <- scaling_weight_sigma
  weight$weight_ratio_mu[p] <- weight_ratio_mu
  weight$weight_ratio_sigma[p] <- weight_ratio_sigma
}

#save data in STAN type of list 

data_weighted <- list(n_stim = n_stim, 
                      n_agents = n_agents, 
                      n_populations = n_populations,
                      max_rating = max_rating,
                      second_rating = second_rating_w,
                      first_rating = first_rating_w, 
                      group_rating = group_rating_w)

rm(first_rating_w, second_rating_w, group_rating_w, expected_rate_w, 
   alpha_post_w, beta_post_w, second_ratings, ratings, weight_ratio_mu, 
   weight_ratio_sigma, scaling_weight_mu, scaling_weight_sigma) # clean up
```




# STAN MODELS 
## STAN MODEL of the simple agent
```{r}
stan_model_simple <- "
data {
  int <lower=1> n_stim;
  int <lower=1> n_agents;
  int <lower=1> max_rating;
  array[n_agents, n_stim] int <lower=0, upper=7> first_rating;
  array[n_agents, n_stim] int <lower=0, upper=7> second_rating;
  array[n_agents, n_stim] int <lower=0, upper=7> group_rating;
}
parameters{
  real <lower=0, upper=10> alpha_prior;
  real <lower=0, upper=10> beta_prior;
}
model {

  for (i in 1:n_agents) {
    for (j in 1:n_stim) {
      real alpha_post = alpha_prior + first_rating[i, j] + group_rating[i, j];
      real beta_post = beta_prior + (max_rating - first_rating[i, j]) + (max_rating - group_rating[i, j]);
      
      // model the second rating
      target += beta_binomial_lpmf(second_rating[i, j] | 7, alpha_post, beta_post);
    }
  }
}
generated quantities {

  array[n_agents, n_stim] real log_lik;
  array[n_agents, n_stim] int prior_pred_rating;
  array[n_agents, n_stim] int post_pred_rating;

  for (i in 1:n_agents) {
    for (j in 1:n_stim) {
      
      prior_pred_rating[i, j] = beta_binomial_rng(7, 1, 1);
      
      real alpha_post = alpha_prior + first_rating[i, j] + group_rating[i, j];
      real beta_post = beta_prior+(max_rating - first_rating[i, j]) + (max_rating - group_rating[i, j]);
      
      post_pred_rating[i, j] = beta_binomial_rng(7, alpha_post, beta_post);
      
      log_lik[i, j] = beta_binomial_lpmf(second_rating[i, j] | 7, alpha_post, beta_post);
    }
  }
}
"
write_stan_file(
  stan_model_simple, dir = "stan/", basename = "simple_agent.stan")
rm(stan_model_simple) #clean up
```

## STAN MODEL of the weighted agent model
```{r}
stan_model_weighted <- "
data {
  int <lower=1> n_stim;
  int <lower=1> n_populations;
  int <lower=1> n_agents;
  int <lower=1> max_rating;
  array[n_populations, n_agents, n_stim] int <lower=0, upper=7> first_rating;
  array[n_populations, n_agents, n_stim] int <lower=0, upper=7> second_rating;
  array[n_populations, n_agents, n_stim] int <lower=0, upper=7> group_rating;
}
parameters {
  // population level params
  array[n_populations] real <lower=-6, upper=10> scaling_mu;
  array[n_populations] real <lower=0> scaling_sigma;
  array[n_populations] real <lower=-6, upper=6> weight_ratio_mu;
  array[n_populations] real <lower=0> weight_ratio_sigma;
  
  // Individual-level (random) effects
  array[n_populations, n_agents] real z_weight_ratio;              
  array[n_populations, n_agents] real z_scaling;                   
}
transformed parameters {
  // Individual-level parameters
  array[n_populations, n_agents] real <lower=0> weight_ratio;  
  array[n_populations, n_agents] real <lower=0> scaling_factor;     
  array[n_populations, n_agents] real <lower=0> weight_first;     
  array[n_populations, n_agents] real <lower=0> weight_group;      
  
  // Non-centered parameterization
  for (p in 1:n_populations){
    for (i in 1:n_agents){
      weight_ratio[p, i] = exp(weight_ratio_mu[p] + z_weight_ratio[p, i] * weight_ratio_sigma[p]);
      scaling_factor[p, i] = exp(scaling_mu[p] + z_scaling[p, i] * scaling_sigma[p]);
      
      weight_first[p, i] = scaling_factor[p, i] * weight_ratio[p, i] / (1 + weight_ratio[p, i]);
      weight_group[p, i] = scaling_factor[p, i]  / (1 + weight_ratio[p, i]);
    }
  }
}
model {
  for (p in 1:n_populations){
    // Population level priors 
    target += normal_lpdf(weight_ratio_mu[p] | 0, 1);
    target += normal_lpdf(scaling_mu[p] | 0, 1);
    target += exponential_lpdf(weight_ratio_sigma[p] | 2);        
    target += exponential_lpdf(scaling_sigma[p] | 2);    
    
    for (i in 1:n_agents){
      // Agent level priors 
      target += std_normal_lpdf(z_weight_ratio[p, i]);
      target += std_normal_lpdf(z_scaling[p, i]);
    }
  }
  // Likelihood
  for (p in 1:n_populations){
    for (i in 1:n_agents){
      real w_first = weight_first[p, i];
      real w_group = weight_group[p, i];
      
      for (j in 1:n_stim){
        real first_rating_w = first_rating[p, i, j] * w_first;
        real group_rating_w = group_rating[p, i, j] * w_group;
        
        real neg_first_rating_w = (max_rating - first_rating[p, i, j]) * w_first;
        real neg_group_rating_w = (max_rating - group_rating[p, i, j]) * w_group;
        
        real alpha_post = 1 + first_rating_w + group_rating_w;
        real beta_post = 1 + neg_first_rating_w + neg_group_rating_w;
        
        target += beta_binomial_lpmf(second_rating[p, i, j] | 7, alpha_post, beta_post);
      }
    }
  }
}
generated quantities {
  
  // Arrays for converted population parameters 
  array[n_populations] real population_ratio;
  array[n_populations] real population_scaling;
  array[n_populations] real population_weight_first;
  array[n_populations] real population_weight_group;

  array[n_populations, n_agents, n_stim] real log_lik;  // Log likelihood for model comparison
  array[n_populations, n_agents, n_stim] int pred_second_rating; // Population and individual predictions
  
  for (p in 1:n_populations){
    population_ratio[p] = exp(weight_ratio_mu[p]);
    population_scaling[p] = exp(scaling_mu[p]);
    population_weight_first[p] = population_scaling[p] * population_ratio[p] / (1 + population_ratio[p]);
    population_weight_group[p] = population_scaling[p] / (1 + population_ratio[p]);
  
    for (i in 1:n_agents){
      real w_first = weight_first[p, i];
      real w_group = weight_group[p, i];
      
      for (j in 1:n_stim){
        // calculate weighted information 
        real first_rating_w = first_rating[p, i, j] * w_first;
        real group_rating_w = group_rating[p, i, j] * w_group;
        real neg_first_rating_w = (max_rating - first_rating[p, i, j]) * w_first;
        real neg_group_rating_w = (max_rating - group_rating[p, i, j]) * w_group;
        real alpha_post = 1 + first_rating_w + group_rating_w;
        real beta_post = 1 + neg_first_rating_w + neg_group_rating_w;
        
        // Generate predictions using beta-binomial
        pred_second_rating[p, i, j] = beta_binomial_rng(7, alpha_post, beta_post);
        
        // Calculate log likelihood
        log_lik[p, i, j] = beta_binomial_lpmf(second_rating[p, i, j] | 7, alpha_post, beta_post);        
      }
    }
  }
}
"
write_stan_file(
  stan_model_weighted, dir = "stan/", basename = "weighted_agent.stan")
rm(stan_model_weighted) #clean up
```



# COMPILE AND FIT MODELS: SIMULATED DATA
## Compile and fit simple model
```{r}
file_simple <- file.path("stan/simple_agent.stan")
mod_simple <- cmdstan_model(file_simple, 
                            cpp_options = list(stan_threads = TRUE))

samples_simple <- mod_simple$sample(
  data = data_simple,
  seed = 120,
  refresh = 500,
  chains = 1, #2
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
simple_sum <- samples_simple$summary()
draws_df_simple <- as_draws_df(samples_simple$draws())
```



```{r}
post_preds_s <- simple_sum[grepl("post_pred_rating", simple_sum$variable) == T,
                         c("variable", "median")]

post_preds_s$agent_id <- str_remove(str_split_i(post_preds_s$variable, ",", 1), "post_pred_rating\\[|")
post_preds_s$face_id <- str_remove(str_split_i(post_preds_s$variable, ",", 2), "\\]")
post_preds_s$median <- as.integer(post_preds_s$median)




plot(array(post_preds_s$median, dim = c(40, 153)), 
     (data_simple$second_rating))

plot(density(abs(post_preds_s$median[post_preds_s$agent_id == 1] data_simple$second_rating[1,])))
```


## Compile and fit weighted model
```{r}
file_weighted <- file.path("stan/weighted_agent.stan")
mod_weighted <- cmdstan_model(file_weighted, 
                              cpp_options = list(stan_threads = TRUE))

samples_weighted <- mod_weighted$sample(
  data = data_weighted,
  seed = 120,
  refresh = 500,
  chains = 1,#2
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
weighted_sum <- samples_weighted$summary()
draws_df_weighted <- as_draws_df(samples_weighted$draws())
```

## Convergence checks for simple and weighted models 
```{r}
cat("Convergence checks for simple and weighted model \n")
simple_rhat_issues <- simple_sum %>%
  filter(rhat > 1.05) %>% nrow()
cat("Simple model parameters with Rhat > 1.05:", 
    simple_rhat_issues, "out of", nrow(simple_sum), "\n")

weighted_rhat_issues <- weighted_sum %>%
  filter(rhat > 1.05) %>% nrow()
cat("Weighted model parameters with Rhat > 1.05:", 
    weighted_rhat_issues, "out of", nrow(weighted_sum), "\n")
```

# FIT MODELS: REAL DATA

## Load data and format to list
(as the weighted model is multilevel, and the simple model isnt (i.e. one has 3D arrays and one hasnt), two different data versions are made)
```{r}
# Load data
data <- read_csv("data/Simonsen_clean.csv", show_col_types = F,
         col_select = c("ID", "FaceID",
                        "FirstRating", "GroupRating", "SecondRating"))
# save data in STAN-friendly list 
data_real <- list(n_stim = length(unique(data$FaceID)), 
                  n_agents = length(unique(data$ID)), 
                  max_rating = 7,
                  second_rating = NA,
                  first_rating = NA, 
                  group_rating = NA)

# make a data version for each of the models : simple
data_real_simple <- data_real
data_real_simple$second_rating <- array(data$SecondRating-1, 
                                 dim = c(data_real$n_agents, data_real$n_stim))
data_real_simple$first_rating <- array(data$FirstRating-1, 
                                 dim = c(data_real$n_agents, data_real$n_stim))
data_real_simple$group_rating <- array(data$GroupRating-1, 
                                 dim = c(data_real$n_agents, data_real$n_stim))

# make a data version for each of the models : weighted
data_real_weighted <- data_real
data_real_weighted$n_populations <- 1
data_real_weighted$second_rating <- array(
  data$SecondRating-1, dim = c(1, data_real$n_agents, data_real$n_stim))
data_real_weighted$first_rating <- array(
  data$FirstRating-1, dim = c(1, data_real$n_agents, data_real$n_stim))
data_real_weighted$group_rating <- array(
  data$GroupRating-1, dim = c(1, data_real$n_agents, data_real$n_stim))
rm(data_real, data) #clean up
```


## Fit simple and weighted model to real data
```{r}
samples_simple_DR <- mod_simple$sample(
  data = data_real_simple,
  seed = 120,
  refresh = 500,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
simple_sum_DR <- samples_simple_DR$summary()
draws_simple_DR <- as_draws_df(samples_simple_DR$draws()) 
```


```{r}
samples_weighted_DR <- mod_weighted$sample(
  data = data_real_weighted,
  seed = 120,
  refresh = 500,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
weighted_sum_DR <- samples_weighted_DR$summary()
draws_weighted_DR <- as_draws_df(samples_weighted_DR$draws())
```
## Convergence checks for simple and weighted models: REAL DATA
```{r}
cat("Convergence checks for simple and weighted model (fit to real data) \n")
simple_rhat_issues_DR <- simple_sum_DR %>%
  filter(rhat > 1.05) %>% nrow()
cat("Simple model parameters with Rhat > 1.05:", 
    simple_rhat_issues_DR, "out of", nrow(simple_sum), "\n")

weighted_rhat_issues_DR <- weighted_sum_DR %>%
  filter(rhat > 1.05) %>% nrow()
cat("Weighted model parameters with Rhat > 1.05:", 
    weighted_rhat_issues_DR, "out of", nrow(weighted_sum), "\n")
```



# MODEL QUALITY CHECKS 


# MODEL COMPARISON

## LOO-CV
```{r}
loo_simple <- samples_simple_DR$loo()
loo_weighted <- samples_weighted_DR$loo()
loo_comparison <- loo_compare(list(simple = loo_simple, weighted = loo_weighted))
print(loo_comparison)
```

## Model Weights
```{r}
model_weights <- loo_model_weights(list(
  "Simple Integration" = loo_simple,
  "Weighted Integration" = loo_weighted))

# Print model weights
print(model_weights)
```

## Visualize model weights
```{r}
model_comp_data <- tibble(
  model = names(model_weights),
  weight = as.numeric(model_weights))

p_model_comp <- ggplot(model_comp_data, aes(x = model, y = weight, fill = model)) +
  geom_col() +
  geom_text(aes(label = scales::percent(weight, accuracy = 0.1)), 
            vjust = -0.5, size = 5) +
  labs(title = "Model Comparison Using LOO-CV",
       subtitle = "Higher weights indicate better predictive performance",
       x = NULL, y = "Model Weight") +
  scale_fill_brewer(palette = "Accent") +
  theme_minimal() +   ylim(0,0.7)+
  theme(legend.position = "none");p_model_comp
```

```{r}
# Compare model performance by agent type
# Calculate pointwise ELPD values for each model
elpd_simple <- loo_simple$pointwise[, "elpd_loo"]
elpd_weighted <- loo_weighted$pointwise[, "elpd_loo"]

# Aggregate by agent
elpd_by_agent <- multilevel_sim_data %>%
  dplyr::select(agent_id, model_type) %>%
  distinct() %>%
  mutate(
    elpd_simple = NA_real_,
    elpd_weighted = NA_real_
  )

# Calculate ELPD sums by agent
for (j in 1:nrow(elpd_by_agent)) {
  agent <- elpd_by_agent$agent_id[j]
  # Find rows for this agent
  agent_rows <- which(multilevel_sim_data$agent_id == agent)
  # Sum ELPD values for this agent
  elpd_by_agent$elpd_simple[j] <- sum(elpd_simple[agent_rows])
  elpd_by_agent$elpd_weighted[j] <- sum(elpd_weighted[agent_rows])
}

# Calculate ELPD difference (positive = weighted model is better)
elpd_by_agent <- elpd_by_agent %>%
  mutate(
    elpd_diff = elpd_weighted - elpd_simple,
    better_model = ifelse(elpd_diff > 0, "Weighted", "Simple")
  )

# Create visualization of model preference by agent type

p_agent_comp <- ggplot(elpd_by_agent, aes(x = agent_id, y = elpd_diff, color = model_type)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Model Preference by Agent Type",
    subtitle = "Positive values favor the weighted model; negative values favor the simple model",
    x = "True Agent Type",
    y = "ELPD Difference (Weighted - Simple)",
    color = "Preferred Model"
  ) +
  theme_minimal()


print(p_agent_comp)
```






