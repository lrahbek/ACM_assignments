---
title: "assignment2_notes"
output: html_document
date: "2025-02-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Install Packages 

```{r}
#Install rstan if not installed: 
#install.packages("rstan", repos = c("https://mcstan.org/rpackages/", getOption ("repos")))
library(rstan)

pacman::p_load(tidyverse, here, posterior, cmdstanr, brms, tidybayes)
```

# Agent functions 
```{r}
# win-stay lose-shift model (with noise)
WSLS_asym_noise <- function(prev_choice, feedback, WS_weight, LS_weight, noise){
  if(feedback == 1){  # win
    if(rbinom(1, 1, WS_weight) == 1){
      choice <- prev_choice
    } else{
      choice <- 1 - prev_choice
    }
  }
  else if (feedback == 0){  # loss
    if(rbinom(1, 1, LS_weight) == 1){
      choice <- 1 - prev_choice
    } else{
      choice <- prev_choice
    }  
  }
  # introduce noise
  if(rbinom(1, 1, noise) == 1){
    choice <- rbinom(1, 1, .5)
  }
  return(choice)
}

# random agent 
random_agent <- function(rate){
  choice <- rbinom(1, 1, rate) 
  return(choice)
} 
```

# STAN CODE 
This version can be compiled 
```{r}
stan_model <- "
data{
  // the number of trials (n) cannot be below 1 and should be integers
  int <lower=1> n;
  
  // the choice (c) have length of number of trials and should be integers either 0 and 1 
  array[n] int <lower=0, upper=1> choice;
  
  // the feedback (f) can either be 0 (loss) or 1 (win)
  array[n] int <lower=0, upper=1> feedback;
  
}

parameters{
  // theta_win (the bias for staying when winning) can be real numbers between 0 and 1
  real <lower=0, upper=1> theta_WS;

  // theta_loss (the for leaving when losing) can be real numbers between 0 and 1 
  real <lower=0, upper=1> theta_LS;

}

transformed parameters{
  vector[n] stay_choice;
  vector[n] lose_choice;
  
  //real theta;
  vector[n] theta;
  
  for (trial in 1:n){
  if(feedback[trial] == 1){
    lose_choice[trial] = 0;
    
  if(choice[trial] == 1){
    stay_choice[trial] = 1;
  }
  else if (choice[trial] == 0){
    stay_choice[trial] = -1;
  }
  }
  if(feedback[trial] == 0){
    stay_choice[trial] = 0;
    
  if(choice[trial] == 1){
    lose_choice[trial] = -1;
  }
  else if (choice[trial] == 0){
    lose_choice[trial] = 1;
  }
  }
  }
  theta = theta_WS*stay_choice + theta_LS*lose_choice;
}

model{
  //priors (same priors for both biases, but individual)
  target += normal_lpdf(theta_WS | 0, 1);
  target += normal_lpdf(theta_LS | 0, 1);
  
  //liklihood (model)
  target += bernoulli_logit_lpmf(choice | theta);
}
"
write_stan_file(
  stan_model, dir = "stan/", basename = "WSLSasym.stan")
```


# SIMULATE DATA 
```{r}
set.seed(123)

n_trials <- 120
rate <- 0.8

WS_weight <- 0.8
LS_weight <- 0.3
noise <- 0.1

choices <- numeric(n_trials)
feedbacks <- rbinom(n_trials, 1, 0.5) # random wins/losses

choices[1] <- random_agent(rate)

for (t in 2:n_trials) {
  choices[t] <- WSLS_asym_noise(choices[t - 1], feedbacks[t - 1], WS_weight, LS_weight, noise)
}

df <- tibble(
  trial = 1:n_trials,
  choice = choices,
  feedback = feedbacks)

data <- list(
  n = n_trials,
  choice = df$choice,
  feedback = df$feedback
)

```


# COMPILE MODEL 
```{r}
file <- file.path("stan/WSLSasym.stan")
mod <- cmdstan_model(file, cpp_options = list(stan_threads = TRUE))

samples <- mod$sample(
  data = data,
  seed = 123,
  refresh = 500,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
```

```{r}
samples$summary()

```


# ASSESS MODEL QUALITY

Lets check the quality of the estimation - the markov chains, and how the prior and the posterior estimates relate to each other.
```{r}
draws_df <- as_draws_df(samples$draws())
```

# Checking the model's chains for both WS and LS theta
```{r}

ggplot(draws_df, aes(.iteration, theta_WS, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()

ggplot(draws_df, aes(.iteration, theta_LS, group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()
```

Both of them looks fine, as we have defined them to be between 0 and 1.
However, lets look the parameters on a logit scale:

```{r}
ggplot(draws_df, aes(.iteration, logit_scaled(theta_WS), group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()

ggplot(draws_df, aes(.iteration, logit_scaled(theta_LS), group = .chain, color = .chain)) +
  geom_line() +
  theme_classic()
```

Great! Here, we can see that the theta_WS goes to +infinity and theta_LS goes to -infinity.

```{r}
# add a prior for theta
draws_df <- draws_df %>% mutate(
  theta_prior = rbeta(nrow(draws_df), 1, 1)
)

# Now let's plot the density for theta (prior and posterior)
ggplot(draws_df) +
  geom_density(aes(theta_WS), fill = "blue", alpha = 0.3) +
  geom_density(aes(theta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.8, # 0.8 is the real WS_weight from the data simulation
             linetype = "dashed", color = "black", linewidth = 1.5) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(theta_LS), fill = "blue", alpha = 0.3) +
  geom_density(aes(theta_prior), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.3, # 0.3 is the real WL_weight from the data simulation
             linetype = "dashed", color = "black", linewidth = 1.5) +
  xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()

```


For WS, the model over-estimates a bit, whereas it under-estimates for WL.

Now, we'll need to add generated quantities to our STAN model in order to calculate the priors and posterior predictions:

```{r}
stan_model_2 <- "
data{
  // the number of trials (n) cannot be below 1 and should be integers
  int <lower=1> n;
  
  // the choice (c) have length of number of trials and should be integers either 0 and 1 
  array[n] int <lower=0, upper=1> choice;
  
  // the feedback (f) can either be 0 (loss) or 1 (win)
  array[n] int <lower=0, upper=1> feedback;
  
}

parameters{
  // logit scale, no defined bondaries
  real theta_WS;
  real theta_LS;
}

transformed parameters{
  vector[n] stay_choice;
  vector[n] lose_choice;
  
  //real theta;
  vector[n] theta;
  
  for (trial in 1:n){
  if(feedback[trial] == 1){
    lose_choice[trial] = 0;
    
  if(choice[trial] == 1){
    stay_choice[trial] = 1;
  }
  else if (choice[trial] == 0){
    stay_choice[trial] = -1;
  }
  }
  if(feedback[trial] == 0){
    stay_choice[trial] = 0;
    
  if(choice[trial] == 1){
    lose_choice[trial] = -1;
  }
  else if (choice[trial] == 0){
    lose_choice[trial] = 1;
  }
  }
  }
  theta = theta_WS*stay_choice + theta_LS*lose_choice;
}

model{
  //priors (same priors for both biases, but individual)
  target += normal_lpdf(theta_WS | 0, 1);
  target += normal_lpdf(theta_LS | 0, 1);
  
  //liklihood (model)
  target += bernoulli_logit_lpmf(choice | theta);
}

generated quantities {
  // obs, this model starts on logit scale and then converts them into probability space
  
  real<lower=0, upper=1> theta_WS_prior = inv_logit(normal_rng(0,1));
  real<lower=0, upper=1> theta_WS_posterior = inv_logit(theta_WS);

  real<lower=0, upper=1> theta_LS_prior = inv_logit(normal_rng(0,1));
  real<lower=0, upper=1> theta_LS_posterior = inv_logit(theta_LS);
  
  // prior
  real theta_WS_prior_logit, theta_LS_prior_logit;
  
  vector[n] theta_prior_pred_logit;
  vector[n] theta_prior_preds;
  
  theta_WS_prior_logit = normal_rng(0,1);
  theta_LS_prior_logit = normal_rng(0,1);
  
  theta_prior_pred_logit = theta_WS_prior_logit*stay_choice + theta_LS_prior_logit*lose_choice;
  
  vector[n] theta_prior_pred_p = inv_logit(theta_prior_pred_logit);
  
  for (i in 1:n){
   theta_prior_preds[i] = binomial_rng(n, theta_prior_pred_p[i]);
  }
  
  // posterior
  vector[n] theta_posterior_preds;
  
  for (i in 1:n){
  theta_posterior_preds[i] = bernoulli_logit_rng(theta[i]);
  }
  
  
}
"

write_stan_file(stan_model_2, dir = "stan/", basename = "WSLSasym_2.stan")
```

```{r}
file_2 <- file.path("stan/WSLSasym_2.stan")
mod_2 <- cmdstan_model(file_2, cpp_options = list(stan_threads = TRUE))

samples_2 <- mod_2$sample(
  data = data,
  seed = 123,
  refresh = 500,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 1000,
  iter_sampling = 2000,
  max_treedepth = 20,
  adapt_delta = 0.99
)
```

```{r}
samples_2$summary('theta_posterior_preds')

```


# PIOR PREDICTIVE CHECKS

```{r}
draws_df_2 <- as_draws_df(samples_2$draws())

# prob. space
ggplot(draws_df_2) +
 geom_histogram(aes(`theta_prior_pred_p[100]`), color="darkblue", 
fill="blue", alpha=0.3, bins=90) +
 geom_point(x = sum(data$choice), y = 0, color="red", shape = 17, 
size = 5) +
 xlab("Predicted right hands out of 120 trials") +
 ylab("Prior count") +
 theme_classic()

# logit space
ggplot(draws_df_2) +
 geom_histogram(aes(`theta_prior_preds[100]`), color="darkblue", 
fill="blue", alpha=0.3, bins=90) +
 geom_point(x = sum(data$choice), y = 0, color="red", shape = 17, 
size = 5) +
 xlab("Predicted right hands out of 120 trials") +
 ylab("Prior count") +
 theme_classic()
```

Before the model sees the data, the model thinks that all values are propable but especially values "in the middle" is likely. The extreme values are discounted.


```{r}
# do some data wrangling so I can plot all trials at once

draws_df_2 <- as_draws_df(samples_2$draws())

ggplot(draws_df_2) +
 geom_histogram(aes(`theta_posterior_preds[100]`), color="darkblue", 
fill="blue", alpha=0.3, bins=90) +
 geom_point(x = sum(data$choice), y = 0, color="red", shape = 17, 
size = 5) +
 xlab("Predicted right hands out of 120 trials") +
 ylab("Prior count") +
 theme_classic()

```

# PRIOR-POSTERIOR UPDATES CHECKS
```{r}
ggplot(draws_df_2) +
 geom_density(aes(`theta_posterior_preds[100]`), fill="blue", alpha=0.3) +
 geom_density(aes(`theta_prior_preds[100]`), fill="red", alpha=0.3) +
 geom_vline(xintercept = 0.8) +
 xlab("Rate") +
 ylab("Posterior Density") +
 theme_classic()

```




